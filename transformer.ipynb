{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer for Kwere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import string\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.normalizers import Sequence, Lowercase\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.trainers import BpeTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 48\n",
    "ALPHABET = [i for i in string.ascii_lowercase +string.digits + r' !\"(),-.:;?'+ \"'\"]\n",
    "#[ !\"'(),-.0-9:;?a-z]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer():\n",
    "    # hugging face API code\n",
    "    # is taken and modified from instructor's \n",
    "    # https://colab.research.google.com/drive/1-TgwCXqYd8ON-58TFzLk413mEqC-7r5F?usp=sharing#scrollTo=0AetkU9nu8OD\n",
    "    \n",
    "    def __init__(self,):\n",
    "        self.tokenizer = Tokenizer(BPE(\n",
    "        )) #byte pair encoding\n",
    "        self.tokenizer.normalizer = Sequence([Lowercase()])  # normalization\n",
    "        self.tokenizer.pre_tokenizer = ByteLevel() #pre-tokenizer\n",
    "        self.tokenizer.decoder = ByteLevelDecoder() #decoder\n",
    "\n",
    "    def bpe_train(self, paths):\n",
    "        trainer = BpeTrainer(vocab_size=VOCAB_SIZE,\n",
    "        initial_alphabet=ALPHABET,\n",
    "        )\n",
    "        self.tokenizer.train(paths, trainer)\n",
    "\n",
    "    def save_tokenizer(self, location, prefix=None):\n",
    "        if not os.path.exists(location):\n",
    "            os.makedirs(location)\n",
    "        self.tokenizer.model.save(location, prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kwere_path = [\"./train-04/cwe-train.txt\"]\n",
    "swahili_path = [\"./train-04/sw-train.txt\"]\n",
    "tokenizer = BPETokenizer()\n",
    "tokenizer.bpe_train(kwere_path)\n",
    "tokenizer.save_tokenizer(\"pretrained_cwe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import GPT2Config, TFGPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./pretrained_cwe/\")\n",
    "tokenizer.add_special_tokens({\n",
    "    \"pad_token\": \"<pad>\",\n",
    "    \"mask_token\": \"<mask>\"\n",
    "})\n",
    "config = GPT2Config(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    n_embd=256,\n",
    "    n_layer=4,\n",
    "    n_head=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tfgpt2lm_head_model_26\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer (TFGPT2MainLaye  multiple                 3434496   \n",
      " r)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,434,496\n",
      "Trainable params: 3,434,496\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# creating the model\n",
    "model = TFGPT2LMHeadModel(config)\n",
    "inputs = tokenizer.encode(\n",
    "    \"kuishi vema maisha ya kikristo\", return_tensors=\"tf\")\n",
    "model(inputs)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_string = ''\n",
    "for filename in kwere_path:\n",
    "  with open(filename, \"r\", encoding='utf-8') as f:\n",
    "    x = f.read()\n",
    "  single_string += x + tokenizer.eos_token\n",
    "string_tokenized = tokenizer.encode(single_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "block_size = 100\n",
    "BATCH_SIZE = 12\n",
    "BUFFER_SIZE = 1000\n",
    "for i in range(0, len(string_tokenized) - block_size + 1, block_size):\n",
    "  examples.append(string_tokenized[i:i + block_size])\n",
    "inputs, labels = [], []\n",
    "for ex in examples:\n",
    "  inputs.append(ex[:-1])\n",
    "  labels.append(ex[1:])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining our optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "# definining our loss function\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# defining our metric which we want to observe\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "# compiling the model\n",
    "model.compile(optimizer=optimizer, loss=[\n",
    "              loss, *[None] * model.config.n_layer], metrics=[metric])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "502/502 [==============================] - 169s 317ms/step - loss: 2.2900 - accuracy: 0.3271\n",
      "Epoch 2/10\n",
      "502/502 [==============================] - 166s 330ms/step - loss: 2.0325 - accuracy: 0.3457\n",
      "Epoch 3/10\n",
      "502/502 [==============================] - 155s 308ms/step - loss: 1.9837 - accuracy: 0.3555\n",
      "Epoch 4/10\n",
      "502/502 [==============================] - 161s 320ms/step - loss: 1.9316 - accuracy: 0.3702\n",
      "Epoch 5/10\n",
      "502/502 [==============================] - 150s 298ms/step - loss: 1.8669 - accuracy: 0.3935\n",
      "Epoch 6/10\n",
      "502/502 [==============================] - 162s 322ms/step - loss: 1.8060 - accuracy: 0.4138\n",
      "Epoch 7/10\n",
      "502/502 [==============================] - 152s 302ms/step - loss: 1.7523 - accuracy: 0.4308\n",
      "Epoch 8/10\n",
      "502/502 [==============================] - 163s 325ms/step - loss: 1.7062 - accuracy: 0.4470\n",
      "Epoch 9/10\n",
      "502/502 [==============================] - 148s 295ms/step - loss: 1.6628 - accuracy: 0.4624\n",
      "Epoch 10/10\n",
      "502/502 [==============================] - 151s 301ms/step - loss: 1.6231 - accuracy: 0.4759\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 10\n",
    "history = model.fit(dataset, epochs=num_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
