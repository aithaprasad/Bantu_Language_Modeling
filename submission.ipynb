{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-10-27 22:46:55.217018: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-27 22:47:00.714366: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFGPT2Model.\n",
      "\n",
      "All the layers of TFGPT2Model were initialized from the model checkpoint at kwere_lm.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFGPT2Model, GPT2Tokenizer\n",
    "import numpy as np\n",
    "kw_test = open(\"test04/cwe-test.txt\").read()\n",
    "kw_model = TFGPT2Model.from_pretrained(\"kwere_lm\")\n",
    "kw_tok = GPT2Tokenizer.from_pretrained(\"kwere_tok\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from architecture import get_tensor_data\n",
    "import tensorflow as tf\n",
    "kw_encoded = kw_tok.encode(kw_test)\n",
    "kwere_test = get_tensor_data(kw_encoded, 5, 28)\n",
    "# defining our optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "# definining our loss function\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# compiling the model\n",
    "kw_model.compile(optimizer=optimizer, loss=[\n",
    "              loss, *[None] * kw_model.config.n_layer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12343/12343 [==============================] - 77s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "outputs = kw_model.predict(kwere_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.061932"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.mean(i)\n",
    "        for i in -np.log2(tf.nn.softmax(outputs.last_hidden_state)[:, -1, :])])\n",
    "#softmax for probability.\n",
    "#take the mean (for batch1, batch2, batch3,... batch 256)... im not sure... This is probably wrong...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12343"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw_test = open(\"test04/sw-test.txt\").read()\n",
    "sw_encoded = kw_tok.encode(sw_test) #same tokenizer\n",
    "kwere_test = get_tensor_data(kw_encoded, 5, 28)\n",
    "# defining our optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "# definining our loss function\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# compiling the model\n",
    "kw_model.compile(optimizer=optimizer, loss=[\n",
    "    loss, *[None] * kw_model.config.n_layer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.571005"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
